---
title: "Logistic Regression Classification, Repeated Cross-Validation, & Permutation Testing"
author: "Ian Douglas"
date: "11/15/2019"
output: html_document
---
```{r, results='hide'}
require(tidyverse)
require(caret)
require(beepr)
```

### Prepare the data
```{r, eval=FALSE, echo=FALSE}
fcon = readRDS('~/DANL/SB/data/processed/labelledFCor.rds')
fdisBeta1 = readRDS('~/DANL/SB/data/processed/labelledFDissBeta1.rds')
# read in PCA data
pc.fc = readRDS('~/DANL/SB/data/processed/fcPCASCoresLbl.rds')
pc.fdBeta1 = readRDS('~/DANL/SB/data/processed/fdBeta1PCASCoresLbl.rds')
topPC.fc = pc.fc[ , -c(grep("^PC36$", names(pc.fc)):ncol(pc.fc))]
topPC.fdBeta1 = pc.fdBeta1[ , -c(grep("^PC36$", names(pc.fdBeta1)):ncol(pc.fdBeta1))]
# remove pc.fc and pc.fdBeta1 from which best PCs were retained
rm(list = c("pc.fc","pc.fdBeta1"))
# Structural data
lblStrData = readRDS("~/DANL/SB/data/processed/structuralLabelled.rds")
lblStrData_noWBV = select(lblStrData,-EstimatedTotalIntraCranialVol)
# Now structural PCA
lblStrPCAScores = readRDS("~/DANL/SB/data/processed/strPCAscoresLabelled.rds")
lblStrPCAScores_noWBV = readRDS("~/DANL/SB/data/processed/strPCAscoresLabelled_noWBV.rds")
# retain best PCs:
topStrPCA =lblStrPCAScores[, 
  -c(grep("^PC8$", names(lblStrPCAScores)):tail(grep("^PC",names(lblStrPCAScores)),1))]
topStrPCA_noWBV =lblStrPCAScores_noWBV[, 
  -c(grep("^PC7$", names(lblStrPCAScores_noWBV)):tail(
    grep("^PC", names(lblStrPCAScores_noWBV)),1))]
rm(list=c("lblStrPCAScores","lblStrPCAScores_noWBV"))
```

```{r}
logReg.dataList = list(
  fcon, topPC.fc, fdisBeta1, topPC.fdBeta1, 
  lblStrData, topStrPCA, lblStrData_noWBV, topStrPCA_noWBV
)
# now delete the data from the environment, since they are in the above list
rm(list = c('fcon', 'fdisBeta1', 'topPC.fc', 'topPC.fdBeta1', 
  'lblStrData', 'topStrPCA', 'lblStrData_noWBV', 'topStrPCA_noWBV'))
```

Note, PI are coded as 1, COMP are 0.
```{r, eval=FALSE, echo=FALSE, results='hide'}
# filter/pre-process these datasets in a uniform way:
logReg.dataList = lapply(logReg.dataList, function(x) {
  rownames(x) = x$IDENT_SUBID
  return(
    na.omit(
    x %>% 
      dplyr::select(-one_of("IDENT_SUBID","SUBJECTID_long", 
                            "age", "wave_to_pull", "cbcl_totprob_t"))
  ) %>%
    #binarize GROUP such that PI = 1, COMP = 0.
    mutate(Y = ifelse(GROUP == "PI", 1L, 0L))
  )
})
```

# Prepare function to automate model building, cross validation, and permutation testing.
```{r, eval=FALSE}
# Function computes:
### Repeated (x1000) 70/30 train-test cross-validation
### Fits a logistic regression
### Permutation test p value for model accuracies.

logReg_CVperm = function(.data, reps = 1000) {
  # Produce 1000 unique train-test splits using a new randomization seed each time:
  trains = lapply(1:reps, function(x) {
    set.seed(x)
    # Require that the test set is 50%-50% of each group
    PI.indices = which(.data$GROUP == "PI"); COMP.indices = which(.data$GROUP == "COMP")
    train.size = round(.7*length(PI.indices))
    # sample indices into a training set:
    train = sample(c(sample(PI.indices,size = train.size), 
                     sample(COMP.indices, size = train.size)))
    return(train)
  }) # the result is a list of 1000 unique vectors with training set indices.
  
  # For each of the 1000 train-test split:
  ### (1) fit one logistic regression model to the training set
  ### (2) Compute one prediction accuracy on its corresponding testing set.
  ### (3) 1000 times: 
  ####### (3a): Randomly permute the labels of the test set
  ####### (3b): Recompute prediction accuracy from the model fit in step 1.
  ### (4) Compute the two-tailed 95% confidence interval for the accuracy from step 2.
  resultsList = lapply(trains, function(y) {
    # 1. fit the model:
    mod = glm(Y ~ .-GROUP, data = .data[y, ],
              family = "binomial")
    # Fitting complete.
    
    # Get the model's predictions for the testing set:
    binaryPreds = ifelse(
      predict(mod, newdata = .data[-y, ], type="response") >= .5, 1, 0)
    
    # Format them as factors with all levels in case random sample 
    prediction = factor(ifelse(binaryPreds==1, "PI", "COMP"), 
                        levels = c("PI", "COMP"))
    actual = factor(.data[-y,]$GROUP, levels = c("PI","COMP"))
    # Get the point estimate of the prediction accuracy for this train-test split:
    predictionAccuracy = confusionMatrix(prediction, actual)$overall["Accuracy"]
    
    # Now, Generate the null distribution for the above prediction accuracy:
    ### (1): Set a unique seed and randomly shuffle the testing set's group labels
    ### (2): Recalculate the prediction accuracy using the real predictions
    nullDist = sapply(reps:1, function(z) {
      set.seed(z)
      perm = sample(actual)
      return(confusionMatrix(prediction, perm)$overall["Accuracy"])
    })
    # nullDist is now a vector (length 1000) containing the empirical distribution of accuracies
    
    
    # Store the train-test accuracy from above, and its respective null distribution:
    out = list("accuracy" = predictionAccuracy, "nd" = nullDist)
    return(out)
  }) # resultsList is now a list with 1000 sublists
  
  # To return the resultsList, check that:
  # (1) resultsList has 1000 elements (sublists)
  # (2) The sublists of resultsList have two elements
  # (3) Those elements are a single accuracy, and the 1000-length vector with the null values
  if (length(resultsList) == reps & 
      all(sapply(resultsList, length) == 2) &
      all(sapply(resultsList, function(a) sapply(a, length)) == c(1, reps))) {
    
    # Then return the results corresponding to each dataset passed to RFclassify_CVperm()
    return(resultsList)
  } else break
}
```

## Implement the function
```{r, eval=FALSE}
# Run it on each dataset in the list established above
beepr::beep_on_error(
  logReg_permResults <- lapply(
    logReg.dataList, function(x) logReg_CVperm(.data = x, reps=1000))
)
beepr::beep()
```

# Extract the results
```{r, eval=FALSE}
## Accuracies
Accuracies = lapply(logReg_permResults,function(x) {
  sapply(x, function(y) {
    y$accuracy
  })
})

# Aggregate null distribution
averageNullDist = lapply(logReg_permResults,function(x) {
  nullMatrix = lapply(x, function(y) {
    sort(y$nd)
  })
  apply(Reduce("cbind", nullMatrix), 1, mean)
})

# Permutation p-value (100% - percent of permuted values closer to chance than the observed)/100
perm.pval = lapply(1:8, function(x) {
  # comparing the mean of all 1000 test-set accuracies to the mean (sorted) null distribution
  (1 + sum(averageNullDist[[x]] > mean(Accuracies[[x]])))/(1 + length(averageNullDist[[x]]))
})
```

```{r, echo=FALSE, eval=FALSE}
saveRDS(Accuracies, "../../../output/permutations/logRegtestSetAccuracies.rds")
saveRDS(averageNullDist, "../../../output/permutations/logRegPermAvgNullDist.rds")
saveRDS(perm.pval, "../../../output/permutations/logRegPerm_p.values.rds")
```

```{r, echo=FALSE, eval=TRUE}
Accuracies = readRDS("../../../output/permutations/logRegtestSetAccuracies.rds")
averageNullDist=readRDS("../../../output/permutations/logRegPermAvgNullDist.rds")
perm.pval=readRDS("../../../output/permutations/logRegPerm_p.values.rds")
```

```{r}
as.data.frame(list("data" = c('FC', 'FCPCA', 'FD', 'FDPCA', 'Str', 
                              'StrPCA', 'Str_noWBV', 'StrPCA_noWBV'),
                   "CV_Avg_Acc." = sapply(Accuracies, mean),
                   "CV_Acc_SD" = sapply(Accuracies, sd),
                   "Null_Mean_Acc." = sapply(averageNullDist, mean),
                   "Null_Acc_SD" = sapply(averageNullDist, sd),
                   "p" = unlist(perm.pval)))
```

# Visualize the results
## Compile results into dataframes for plotting
```{r}
## Add some labels and convert from wide to long format to plot distributions
dataNames = c('FC', 'FCPCA', 'FD', 'FDPCA', 'Str',
              'StrPCA', 'Str_noWBV', 'StrPCA_noWBV')
dataType = c(rep(c("connectivity", "dissimilarity"),each=2), rep("structural",4))
perm_plt_data = Reduce("rbind", lapply(1:8, function(x) {
  data.frame("model" = rep(dataNames[x], times= 2000),
             "dataType" = rep(dataType[x], times = 2000),
             "Distribution" = rep(c("Test.Set.Repetitions", "Permuted.Null"), each = 1000),
             "Accuracy" = c(Accuracies[[x]], averageNullDist[[x]]),
             stringsAsFactors = FALSE)
}))

# seprate functional and structural data for plotting
fMRI_plt_data = perm_plt_data %>% 
  filter(dataType != "structural")
StrMRI_plt_data = perm_plt_data %>% 
  filter(dataType == "structural")
```

## Generate plots
```{r}
fMRI_plt = ggplot(fMRI_plt_data, aes(Accuracy, fill = Distribution)) +
  geom_density(alpha = .3) +
  geom_vline( # calculate the means
    data = (
      data.frame("model"=dataNames,"avg" = sapply(Accuracies, mean)) %>%
        filter(grepl("^F", model))
    ),
    aes(xintercept = avg)) +
  facet_grid(~model) +
  ggtitle("Functional Data Model Accuracies and Permutation Test Results") +
  theme(panel.background = element_rect(fill="white"),
        plot.title = element_text(hjust = .5))

StrMRI_plt = ggplot(StrMRI_plt_data, aes(Accuracy, fill = Distribution)) +
  geom_density(alpha = .3) +
  geom_vline( # calculate the means
    data = (
      data.frame("model"=dataNames,"avg" = sapply(Accuracies, mean)) %>%
        filter(!grepl("^F", model))
    ),
    aes(xintercept = avg)) +
  facet_grid(~model) +
  ggtitle("Structural Data Model Accuracies and Permutation Test Results") +
  theme(panel.background = element_rect(fill="white"),
        plot.title = element_text(hjust = .5))
```

```{r, eval=FALSE, echo=FALSE}
ggsave("../../../results/permutations/plots/fMRILogReg_accuracyRFPermResults.pdf",
       plot = fMRI_plt,
       height = 3, width = 8, units = "in", device = "pdf")
ggsave("../../../results/permutations/plots/STRMRILogReg_accuracyRFPermResults.pdf", 
         plot = StrMRI_plt,
         height = 3, width = 8, units = "in", device = "pdf")
```


```{r, fig.width=10,fig.height=3}
fMRI_plt
```
```{r, fig.width=10,fig.height=3}
StrMRI_plt
```



