---
title: "structuralMRI_logistic_regression"
author: "Ian Douglas"
date: "11/15/2019"
output: html_document
---
```{r}
require(tidyverse)
require(caret)
```

### Prepare the data

Note, PI will be recoded as 1, COMP are 0.
```{r}
# conduct this on the structural data for now
logReg.dataList = list(lblStrData_noWBV, lblStrPCAScores_noWBV)
# filter/pre-process these datasets in a uniform way:
logReg.dataList = lapply(logReg.dataList, function(x) {
  rownames(x) = x$IDENT_SUBID
  return(
    na.omit(
    data %>% 
      select(-one_of("IDENT_SUBID","SUBJECTID_long", "age", "wave_to_pull", "cbcl_totprob_t"))
  ) %>%
    #binarize GROUP such that PI = 1, COMP = 0.
    mutate(Y = ifelse(GROUP == "PI", 1L, 0L))
  )
})
```

# Prepare function to automate model building, cross validation, and permutation testing.
```{r, eval=FALSE}
# Function computes:
### Repeated (x1000) 70/30 train-test cross-validation
### Fits a logistic regression
### Permutation test p value for model accuracies.

logReg_CVperm = function(data, reps = 1000) {
  # pre-process the data identically for each dataset
  rownames(data) = data$IDENT_SUBID
  dat = na.omit(
    data %>% 
      select(-one_of("IDENT_SUBID","SUBJECTID_long", "age", "wave_to_pull", "cbcl_totprob_t"))
  ) %>%
    #binarize GROUP such that PI = 1, COMP = 0.
    mutate(Y = ifelse(GROUP == "PI", 1L, 0L))
  
  # Produce 1000 unique train-test splits using a new randomization seed each time:
  trains = lapply(1:reps, function(x) {
    set.seed(x)
    # Require that the test set is 50%-50% of each group
    PI.indices = which(dat$GROUP == "PI"); COMP.indices = which(dat$GROUP == "COMP")
    train.size = round(.7*length(PI.indices))
    # sample indices into a training set:
    train = sample(c(sample(PI.indices,size = train.size), 
                     sample(COMP.indices, size = train.size)))
    return(train)
  }) # the result is a list of 1000 unique vectors with training set indices.
  
  # For each of the 1000 train-test split:
  ### (1) fit one logistic regression model to the training set
  ### (2) Compute one prediction accuracy on its corresponding testing set.
  ### (3) 1000 times: 
  ####### (3a): Randomly permute the labels of the test set
  ####### (3b): Recompute prediction accuracy from the model fit in step 1.
  ### (4) Compute the two-tailed 95% confidence interval for the accuracy from step 2.
  resultsList = lapply(trains, function(y) {
    # 1. fit the model:
    mod = glm(Y ~ .-GROUP, data = dat[y, ],
              family = "binomial")
    # Fitting complete.
    
    # Get the model's predictions for the testing set:
    binaryPreds = ifelse(
      predict(mod, newdata = dat[-y, ], type="response") >= .5, 1, 0)
    
    # Format them as factors with all levels in case random sample 
    prediction = factor(ifelse(binaryPreds==1, "PI", "COMP"), 
                        levels = c("PI", "COMP"))
    actual = factor(dat[-y,]$GROUP, levels = c("PI","COMP"))
    # Get the point estimate of the prediction accuracy for this train-test split:
    predictionAccuracy = confusionMatrix(prediction, actual)$overall["Accuracy"]
    
    # Now, Generate the null distribution for the above prediction accuracy:
    ### (1): Set a unique seed and randomly shuffle the testing set's group labels
    ### (2): Recalculate the prediction accuracy using the real predictions
    nullDist = sapply(reps:1, function(z) {
      set.seed(z)
      perm = sample(actual)
      return(confusionMatrix(prediction, perm)$overall["Accuracy"])
    })
    # nullDist is now a vector (length 1000) containing the empirical distribution of accuracies
    
    
    # Store the train-test accuracy from above, and its respective null distribution:
    out = list("accuracy" = predictionAccuracy, "nd" = nullDist)
    return(out)
  }) # resultsList is now a list with 1000 sublists
  
  # To return the resultsList, check that:
  # (1) resultsList has 1000 elements (sublists)
  # (2) The sublists of resultsList have two elements
  # (3) Those elements are a single accuracy, and the 1000-length vector with the null values
  if (length(resultsList) == reps & 
      all(sapply(resultsList, length) == 2) &
      all(sapply(resultsList, function(a) sapply(a, length)) == c(1, reps))) {
    
    # Then return the results corresponding to each dataset passed to RFclassify_CVperm()
    return(resultsList)
  } else break
}
```

## Implement the function
```{r, eval=FALSE}
# Run it on each dataset in the list established above
beepr::beep_on_error(
  logReg_permResults <- lapply(
    logReg.dataList, function(x) logReg_CVperm(x, reps=1000)),
  2 # choose the beep for the error
)
```

# Extract the results
```{r}
## Accuracies
Accuracies = lapply(logReg_permResults,function(x) {
  sapply(x, function(y) {
    y$accuracy
  })
})

# Aggregate null distribution
averageNullDist = lapply(logReg_permResults,function(x) {
  nullMatrix = lapply(x, function(y) {
    sort(y$nd)
  })
  apply(Reduce("cbind", nullMatrix), 1, mean)
})

# Permutation p-value (100% - percent of permuted values closer to chance than the observed)/100
perm.pval = lapply(1:2, function(x) {
  # comparing the mean of all 1000 test-set accuracies to the mean (sorted) null distribution
  (1 + sum(averageNullDist[[x]] > mean(Accuracies[[x]])))/(1 + length(averageNullDist[[x]]))
})
```

```{r}
as.data.frame(list("data" = c("Str","Str.PCA"),
                   "CV_Avg_Acc." = sapply(Accuracies, mean),
                   "CV_Acc_SD" = sapply(Accuracies, sd),
                   "Null_Mean_Acc." = sapply(averageNullDist, mean),
                   "Null_Acc_SD" = sapply(averageNullDist, sd),
                   "p" = unlist(perm.pval)))
```

